{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 使用神经网络 图像识别入门\n",
    "------\n",
    "\n",
    "**CV，计算机视觉** 是一个应用广泛的方向，而且看起来也挺有趣，由于它的可视化和资料丰富，用来做机器学习的入门是再好不过的了。\n",
    "\n",
    "我们这次分享的内容是使用现有的机器学习库，搭建神经网络，来做图像识别的应用。\n",
    "这是一次应用层上的以项目驱动的入门经验分享，所以内容比较简单，\n",
    "不涉及到神经网络的复杂原理，仅会从使用上介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备工作\n",
    "------\n",
    "\n",
    "## 数据\n",
    "------\n",
    "\n",
    "我们主要的目的是看看图像识别程序是怎么做出来的，所以即使前期的准备工作再重要，也不是这次分享的重点。\n",
    "\n",
    "计算机视觉方向的数据集有很多，这些数据集就是有标注的图片，视频等，用来做监督学习。\n",
    "\n",
    "图像识别方面的数据集有很多，比如 hello world 级别的 **MNIST** 手写数字数据集，数据量非常大的 **ImageNet** 数据集，**VOC PASCAL** 数据集等等。\n",
    "\n",
    "## 环境\n",
    "------\n",
    "\n",
    "机器学习框架一般是以 CUDA 为后端，Python等语言作为前端，通过 C API 作为桥梁，用前端定义计算图，在后端执行并将结果返回前端。\n",
    "\n",
    "虽然各大框架对Python的支持是最好的，但是也是可以使用其他语言来编程的，\n",
    "\n",
    "比如**Torch**支持**lua**，**TensorFlow**还支持**C++,Java,JavaScript**，MXNET支持的更多。\n",
    "\n",
    "python是一个动态类型的脚本语言，写起来简单，也是一个胶水语言，对C API支持的很好。\n",
    "\n",
    "将计算密集型的工作交给CUDA，用python定义模型对于机器学习框架来说是很好的选择。\n",
    "\n",
    "如果安装Python可以选择它的发行版，**Anaconda**。\n",
    "\n",
    "Anaconda集成了很多有用的库，比如numpy，scipy，matpltlib等，而且conda自带包管理工具，很好用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 框架选择\n",
    "\n",
    "![tf_vs_torch](./images/tf_vs_torch_r.jpg)\n",
    "\n",
    "目前机器学习计算框架主要有谷歌开发的TensorFlow，Facebook开发的PyTorch和Caffe2，\n",
    "亚马逊的MXNET等。之前比较火的Theano停止维护，Keras成为TensorFlow的核心包，\n",
    "Caffe2的代码也已经开始并入PyTorch。机器学习框架的竞争可以说是十分激烈，也都越来越好用。\n",
    "\n",
    "目前框架排名前两名是TensorFlow和PyTorch，下面说一下它们的特点。\n",
    "\n",
    "## 静态计算图 vs 动态计算图\n",
    "\n",
    "![静态计算图中的张量流](./images/tensorflow.gif)\n",
    "\n",
    "在TensorFlow的1.4以前的版本，是纯静态图模式的。\n",
    "要先写好所有的计算图，图中的节点只有形状，类型属性，而没有具体的数据，\n",
    "也不需要每个节点都由人工去定义，框架可以推断出来运算结果的形状。\n",
    "运行的话是生成静态的计算图后，再将数据传入进行计算。\n",
    "形容一下写模型的过程，TensorFlow像是编译型的模板引擎，用Python的语法写模板，编译后的程序交给 CUDA 来执行，再将执行结果返回到前端，\n",
    "和写前端模板一样。\n",
    "TensorFlow静态图的优点就是可以方便地导入导出模型，方便部署。\n",
    "\n",
    "1.5版本以后，Tensorflow也支持了动态图模式，Eager execution，像脚本语言一样，一行一行执行，这样对于调试模型非常方便。\n",
    "目前为了调试方便，我写的的模型同时支持静态图和动态图，不过动态图只支持了预测和验证，训练还是静态图，因为动态图写训练非常复杂。\n",
    "\n",
    "------\n",
    "\n",
    "PyTorch是一个动态图框架，也是以张量`Tensor`为基础的一个框架，由于其动态图特点，没有TensorFlow那么多API要记，写起来也更简单。\n",
    "\n",
    "TensorFlow写着不像Python，很麻烦，偏离了Python简单轻便的初衷，PyTorch写出来更简单更好看一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础知识\n",
    "\n",
    "## 张量 Tensor 和 变量 Variable\n",
    "\n",
    "|     张量         |   形状  |   阶  |  别名  |\n",
    "| :----------:     | :-----: | :---: | :----: |\n",
    "|       5          |    []   |   0   |  标量 0-D Tensor  |\n",
    "|   [1,2,3,4]      |   [5]   |   1   |  向量 1-D Tensor |\n",
    "|  [[1,2], [3,4]]  |  [2,2]  |   2   |  矩阵 2-D Tensor |\n",
    "|   [[[0,0,0]]]    | [1,1,3] |   3   |    3-D Tensor    |\n",
    "|      ...         |         |   N   |    N-D Tensor    |\n",
    "\n",
    "张量是标量，向量，矩阵的扩展。\n",
    "\n",
    "## 框架中的Tensor\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "TensorFlow中的Tensor是不可变的，Variable是可变的(mutable) 。\n",
    "Tensor一般是输入，或者经过计算得到的。\n",
    "能用Tensor的地方，也能用Variable。\n",
    "\n",
    "Variable可以赋值，使用assign/assign_add/assign_sub方法可以直接修改变量，Tensor就没有这些方法。\n",
    "\n",
    "静态图中任何的操作都是定义了一个操作，可以类比定义了一个函数，必须在运行环境中执行才会返回结果，Tensorflow静态图的运行环境就是Session。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(2, 3), dtype=int32)\n",
      "Tensor(\"add:0\", shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# 创造一个int32类型的张量，初始值是[[1, 2, 3], [4, 5, 6]], 一个二维的，形状是[2,3]的张量\n",
    "t = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int32)  # 返回是一个操作，这个操作放在会话中运算的结果是[[1, 2, 3], [4, 5, 6]]。\n",
    "print(t)\n",
    "t = t + 1 # 一个加法操作，可以看到它的名称是add，很明显就是一个操作。\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面也说了，对一个张量做任何操作，返回的都是那个操作，而张量本身是不变的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'v:0' shape=(2, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "v = tf.Variable([[1, 2, 3], [4, 5, 6]], dtype=tf.float32, name=\"v\")  # 给variable定义的名字，名字能够唯一确定一个张量或变量\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'v:0' shape=(2, 3) dtype=float32_ref>\n",
      "Tensor(\"AssignAdd:0\", shape=(2, 3), dtype=float32_ref)\n"
     ]
    }
   ],
   "source": [
    "add_op = tf.assign_add(v, tf.ones_like(v))  # 变量也是同样道理，不能直接修改变量的值，对变量的操作也是如此，赋值操作也是同样。\n",
    "print(v)\n",
    "print(add_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里没有真正的加1，而且返回了一个操作`add_op`，这个操作被执行后，v的值才会改变。\n",
    "我们在Session中运行一些就可以看到，名字还是'v:0'，说明该变量和原来是同一个，只是内容发生了变化。\n",
    "\n",
    "------\n",
    "从上面的输出可以看到，每个操作并没有实际的数值属性，也就是没有数据参与计算。运行静态图的方法是开启一个会话，将计算图和数据传入会话中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 't' is:\n",
      "[[2 3 4]\n",
      " [5 6 7]]\n",
      "Variable before add:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "Add_op is:\n",
      "[[2. 3. 4.]\n",
      " [5. 6. 7.]]\n",
      "Variable after add:\n",
      "[[2. 3. 4.]\n",
      " [5. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"Tensor 't' is:\")\n",
    "print(sess.run(t))\n",
    "\n",
    "# 作为常量的张量在会话开始时自动初始化，但是变量不会，所以需要手动初始化，高级API会自动做变量的初始化\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#这里的`v`并没有被+1，因为初始化时就是把定义`v`时的初始值给它，初始化以后再+1就可以得到正确答案了。\n",
    "\n",
    "add_op = tf.assign_add(v, tf.ones_like(v))\n",
    "print(\"Variable before add:\")\n",
    "print(sess.run(v))\n",
    "print(\"Add_op is:\")\n",
    "print(sess.run(add_op))\n",
    "print(\"Variable after add:\")\n",
    "print(sess.run(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "PyTorch和TensorFlow不一样，它的Tensor是可变的，而它的Variable所在包名是`torch.autograd`，auto gradient，自动求梯度。\n",
    "Variable主要包含三个属性。\n",
    "\n",
    "![autograd](./images/autograd_Variable.png)\n",
    "\n",
    "- `data`：保存Variable所包含的Tensor\n",
    "- `grad`：保存`data`对应的梯度，`grad`也是个Variable，而不是Tensor，它和`data`的形状一样。\n",
    "- `grad_fn`：指向一个`Function`对象，这个`Function`用来反向传播计算输入的梯度。\n",
    "\n",
    "我们知道梯度的方向是一个函数在某一点上函数值增加率最大的方向，沿着负梯度方向就是函数值减小最快的方向。\n",
    "沿着负梯度方向走，就能到达函数的一个局部极小值点。在机器学习里使用的是损失函数梯度下降法，当损失函数也就是误差越小，说明模型越准确。\n",
    "\n",
    "![梯度下降](./images/gradient_descent.gif)\n",
    "\n",
    "PyTorch的Variable有个属性叫做`requires_grad`，是否要求这个变量的梯度。\n",
    "当一个变量 `A` 的这个属性是True，那么这个变量参与运算得到的结果 `B` ，也就是由此产生的另一个变量，可以通过执行`B.backward()`方法，自动计算出 A 的梯度 `A.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "t = torch.FloatTensor([1,2,3])  # 创造一个浮点类型的张量，初始值是[1,2,3]\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t[0] = -1  # 对该张量做item assign，ok，说明torch中的张量是可变的\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v1 = Variable(t, requires_grad=True)  # 创造一个变量，初始值是 t，requires_grad 表示当进行反向传播时，要求出这个变量的梯度\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 24\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "grad of v1:  Variable containing:\n",
      " 6\n",
      " 6\n",
      " 6\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "grad of v2:  Variable containing:\n",
      "-1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v2 = Variable(torch.Tensor([6, 6, 6]), requires_grad=True)  # 某张量\n",
    "v3 = torch.dot(v1, v2)  #  做向量内积，结果应该是 -1**6 + 2**6 + 3**6 = 24\n",
    "print(v3)  # 打印出结果，所谓正向传播就是一步一步往下计算\n",
    "\n",
    "v3.backward()  # 将 v3 看成一个函数，即 f(x, y) = x·y, 执行这个反向传播方法，就可以自动算出上一层变量的梯度。\n",
    "\n",
    "print(\"grad of v1: \", v1.grad)\n",
    "print(\"grad of v2: \", v2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度告诉我们，如果想让函数值变大应该怎么修改参数，想让函数值变小就按照负梯度修改参数。\n",
    "比如这里我想让v3的值变小，那么可以设置个学习率lr=0.001，就可以让v1 = v1 + lr * -v1.grad, 再看一下结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.0060\n",
      " 1.9940\n",
      " 2.9940\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      " 23.8920\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "v1.add_(lr * -v1.grad)  # torch的add有两种，一种是`torch.add`，返回一个运算后的值，被调用的张量不变；\n",
    "                        # 还有`add_`, 返回结果并改变被调用的张量的值。\n",
    "print(v1)\n",
    "\n",
    "v3 = torch.dot(v1, v2)\n",
    "print(v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果确实变小了，所以梯度下降确实管用！\n",
    "梯度的方向决定参数值是调大还是调小，绝对值大小决定幅度的大小。\n",
    "这里的学习率就是参数改变的幅度大小，如果设置得大了，可能就过犹不及，函数值反而变大了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归和逻辑回归\n",
    "\n",
    "结果是连续的还是离散的\n",
    "\n",
    "## 图像分类\n",
    "\n",
    "单纯的图像分类是逻辑回归，常用的方法是一个非线性函数包装线性回归的结果，常用的有`sigmoid`函数 `softmax`函数，用于二分类和多分类。\n",
    "\n",
    "## 目标检测\n",
    "\n",
    "目标检测需要画出检测出的目标的边框，矩形边框的四个属性([ymin, xmin, ymax, xmax]或[center_x, center_y, height, width])是连续的，所以目标检测中既有线性回归也有逻辑回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算机视觉中的操作\n",
    "卷积 和 池化 是图像处理中常用操作\n",
    "\n",
    "## 2d卷积\n",
    "\n",
    "conv_2d的卷积核是一个二维张量，一个矩阵，也叫算子。\n",
    "卷积操作就是将卷积核与图片的一个通道(也是二维张量)做如下操作：\n",
    "![卷积](./images/conv.gif)\n",
    "在一个矩阵上圈出一块，和卷积核做一个内积并输出，在圈另一块，和同一个卷积核继续这个操作。\n",
    "上面是一个最简单的卷积操作，还有复杂的depth-wise卷积，point-wise卷积，空洞卷积等。\n",
    "\n",
    "## 2d池化\n",
    "pool_2d的操作很好理解，也是在一个矩阵上圈出一块，做一个计算，和卷积不同的是它经常是求这个小块里的最大值，平均值。\n",
    "![池化](./images/pool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手写数字识别\n",
    "下面开始手写数字识别的代码\n",
    "\n",
    "## MNIST数据集介绍\n",
    "\n",
    "![mnist](./images/mnist_1.png)\n",
    "\n",
    "数字3\n",
    "\n",
    "![mnist](./images/3.jpg)\n",
    "\n",
    "MNIST数据集中的数字图片是单通道灰度图，形状`[28, 28]`，在tensorflow中提供的mnist将这个矩阵展开成了一个`shape=[784]`的向量。\n",
    "要想显示这个图片，可以将这个张量`reshape`，将向量还原成矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "Shape is (784,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2733b8f8128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")  # 读取数据集，本地没有缓存就下载一个\n",
    "\n",
    "train_data = mnist.train.images  # Returns np.ndarray\n",
    "train_labels = mnist.train.labels  # Returns np.ndarray\n",
    "\n",
    "image = train_data[8]\n",
    "label = train_labels[8]\n",
    "\n",
    "print('Shape is {shape}'.format(shape=image.shape))\n",
    "\n",
    "plt.title('Label is {label}'.format(label=label))\n",
    "plt.imshow(image.reshape([28, 28]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全连接神经网络\n",
    "\n",
    "全连接是一种很直观的神经网络\n",
    "![fcnn](./images/fc_nn.png)\n",
    "我们先做一个这样的网络，它有三层全连接。\n",
    "\n",
    "## 全连接层\n",
    "我们知道每个神经元相当于一个函数，这里的神经元就是`f(x)=W * x + b`，这里的`W`和`b`就是需要学习的参数。\n",
    "这里的第一个全连接层相当于把一个长度784的向量依次传入16个函数，那么能得到16个结果，那么有几个神经元，就会输出多少个值。如果还有下一层，这些输出的值就会作为输入，传给下一层的神经元。\n",
    "\n",
    "上图输入层有784个输入，第二，三排各有16个神经元，第四层有10个神经元，代表10个数字，整个神经网络就是一个大方程，输入一个[784]形状的向量，输出一个[10]形状的向量。对输出的这10个数我们还可以做一个`softmax`，我们知道`max`函数是可以返回传入的参数中最大值，\n",
    "`softmax`会计算出每个元素指数占所有元素的指数之和的比例，这是一个逻辑函数的扩展，用于多分类的逻辑回归。\n",
    "\n",
    "![](./images/softmax.jpg)\n",
    "------\n",
    "\n",
    "![estimator](./images/estimator.png)\n",
    "那么就使用Tensorflow的高级API`tf.estimator`包，搭建一个全连接的神经网络吧。\n",
    "\n",
    "tensorflow中提供的卷积层api是`tf.layers.dense`, 密集连接层，和全连接是一个意思。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './mnist_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002733CB383C8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def dnn_model_fn(features, labels, mode):\n",
    "    \"\"\"\n",
    "    用于构造estimator的model_fn\n",
    "    :param features: 输入\n",
    "    :param labels: 标签\n",
    "    :param mode: 模式, 训练/验证/预测\n",
    "    :return: EstimatorSpec\n",
    "    \"\"\"\n",
    "    input_layer = features.get(\"input\")   # 从字典中取到这次训练的图片，我们会拿到两张图，所以input_layer的形状是[2, 784]\n",
    "    labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)  # 将标量的标签转成one-hot形式\n",
    "    \n",
    "    full_connect_layer_1 = tf.layers.dense(inputs=input_layer, units=16)  # 一个有16个神经元的全连接（密集连接）层\n",
    "    full_connect_layer_2 = tf.layers.dense(inputs=full_connect_layer_1, units=16)  # 又一个有16个神经元的全连接层\n",
    "    full_connect_layer_3 = tf.layers.dense(inputs=full_connect_layer_2, units=10)  #  包含10个神经元的全连接层\n",
    "    \n",
    "    softmax = tf.nn.softmax(full_connect_layer_3)  # 转换成概率\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=softmax, axis=1),\n",
    "        \"probabilities\": softmax\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=full_connect_layer_3)  # 损失函数，交叉熵，越小越好\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        train_op = optimizer.minimize(loss, global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    # 剩下就是eval，验证模式\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=tf.argmax(labels, 1),\n",
    "                                        predictions=predictions['classes'],\n",
    "                                        name='accuracy')\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=loss,\n",
    "        eval_metric_ops=eval_metric_ops\n",
    "    )\n",
    "\n",
    "dnn_mnist_classifier = tf.estimator.Estimator(model_fn=dense_model_fn, model_dir=\"./dnn_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mnist.train.images  # 训练集\n",
    "train_labels = mnist.train.labels\n",
    "eval_data = mnist.test.images  # 验证集，用来验证模型的好坏\n",
    "eval_labels = mnist.test.labels\n",
    "\n",
    "\n",
    "#  使用estimator需要定义输入函数，这里tensorflow自带了一个\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                x={\"input\": train_data},  #  输入的图片\n",
    "                y=train_labels,           #  对应的标签\n",
    "                batch_size=2,             #  每次传入一张图片，同时传入多张图片做mini batch训练结果更好\n",
    "                num_epochs=None,          #  将整个数据集循环几次\n",
    "                shuffle=True              #  是否打乱顺序\n",
    "            )\n",
    "\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"input\": eval_data},\n",
    "                    y=eval_labels,\n",
    "                    num_epochs=1,\n",
    "                    shuffle=False\n",
    "                )  # 同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_model\\model.ckpt-9000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 9001 into ./mnist_model\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.039568614, step = 9001\n",
      "INFO:tensorflow:global_step/sec: 729.444\n",
      "INFO:tensorflow:loss = 0.053829417, step = 9101 (0.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 1030.25\n",
      "INFO:tensorflow:loss = 0.10309934, step = 9201 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 960.903\n",
      "INFO:tensorflow:loss = 0.038043384, step = 9301 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 951.741\n",
      "INFO:tensorflow:loss = 0.03310904, step = 9401 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 1074.57\n",
      "INFO:tensorflow:loss = 0.046363212, step = 9501 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 933.965\n",
      "INFO:tensorflow:loss = 0.57677424, step = 9601 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 1086.24\n",
      "INFO:tensorflow:loss = 0.32371923, step = 9701 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1063.12\n",
      "INFO:tensorflow:loss = 0.42038646, step = 9801 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1040.98\n",
      "INFO:tensorflow:loss = 0.26461414, step = 9901 (0.097 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into ./mnist_model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.023495942.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-14:39:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_model\\model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-14:39:53\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.8803, global_step = 10000, loss = 0.42725876\n",
      "{'accuracy': 0.8803, 'loss': 0.42725876, 'global_step': 10000}\n"
     ]
    }
   ],
   "source": [
    "dnn_mnist_classifier.train(input_fn=train_input_fn, steps=10000)  # 训练10000步就可以达到很好的效果, 几秒钟就可以训练好了\n",
    "eval_results = dnn_mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    "\n",
    "一般来说想要识别数字9，要先看到它的上门有个圈，然后下面连一条线。\n",
    "\n",
    "2d卷积神经网络就是将二维结构考虑了进去，上面的卷积示意图中可以看到，经过卷积核的运算得到的每个结果都是周围3\\*3,9个像素共同参与运算得到。\n",
    "\n",
    "我们训练出不同的卷积核来提取不同的特征，比如卷积核1号对圆圈敏感，卷积核2号对竖线敏感，那么数字9和数字2经过这两个卷积核得到的值肯定会有很大差别。\n",
    "\n",
    "卷积层一般也有多层，可以根据输入的张量尺寸，选择不同层数。\n",
    "\n",
    "靠前的卷积核可以提取低级特征，如圆圈的左上角圆弧，竖线的下半部分。\n",
    "\n",
    "靠后的卷积核在前面的基础上，可以提取高级特征，比如可以判断是否有圆圈，是否有竖线。\n",
    "\n",
    "**全连接是一个特殊的卷积，可以看作卷积核的形状和输入形状相同。**\n",
    "\n",
    "tensorflow中提供的卷积层api是`tf.layers.conv2d`，同样使用`estimator`, 看看效果会提高多少吧。\n",
    "我们的`input_fn`和上面的完全相同，所以就不在定义了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './cnn_mnist_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002733EBD8DA0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    input_layer = features.get(\"input\")   # 从字典中取到这次训练的图片，我们会拿到两张图，所以input_layer的形状是[2, 784]\n",
    "    labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)  # 将标量的标签转成one-hot形式\n",
    "    input_layer = tf.reshape(input_layer, [-1, 28, 28, 1])\n",
    "    conv1 = tf.layers.conv2d(inputs=input_layer, filters=32, kernel_size=[5, 5], padding=\"same\")\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[5, 5], padding=\"same\")\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    pool2_flat = tf.reshape(pool2, shape=[-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    \n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        train_op = optimizer.minimize(loss, global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=tf.argmax(labels, 1),\n",
    "                                        predictions=predictions['classes'],\n",
    "                                        name='accuracy')\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=loss,\n",
    "        eval_metric_ops=eval_metric_ops\n",
    "    )\n",
    "\n",
    "cnn_mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"./cnn_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./cnn_mnist_model\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.424192, step = 1\n",
      "INFO:tensorflow:global_step/sec: 25.5455\n",
      "INFO:tensorflow:loss = 1.0320555, step = 101 (3.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.3704\n",
      "INFO:tensorflow:loss = 0.36364695, step = 201 (3.941 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.8592\n",
      "INFO:tensorflow:loss = 0.021345321, step = 301 (4.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 26.0244\n",
      "INFO:tensorflow:loss = 0.4292304, step = 401 (3.842 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.9232\n",
      "INFO:tensorflow:loss = 0.21305872, step = 501 (3.859 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.8027\n",
      "INFO:tensorflow:loss = 0.08759871, step = 601 (3.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.9299\n",
      "INFO:tensorflow:loss = 0.020435255, step = 701 (3.857 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.3382\n",
      "INFO:tensorflow:loss = 0.013053667, step = 801 (3.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 25.8094\n",
      "INFO:tensorflow:loss = 0.021737613, step = 901 (3.876 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./cnn_mnist_model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.2623845.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-15:17:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./cnn_mnist_model\\model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-15:17:17\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.917, global_step = 1000, loss = 0.26926425\n",
      "{'accuracy': 0.917, 'loss': 0.26926425, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "cnn_mnist_classifier.train(input_fn=train_input_fn, steps=1000)  # 训练1000步就可以达到很好的效果, 都是由于计算比较复杂，所需时间长一些 \n",
    "eval_results = cnn_mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST总结\n",
    "上面的两个非常简单的模型就可以达到91%以上的准确率，可以说效果不错了，在cnn_model的基础上再修改一些，可以达到更好的准确率，\n",
    "\n",
    "在卷积层增加激活函数`relu`，全连接层中间加入`dropout`层后，2000步的训练结果就很好了：\n",
    "\n",
    "`{'accuracy': 0.9854, 'loss': 0.04469394, 'global_step': 2000}`\n",
    "\n",
    "这里就不多说了。\n",
    "模型可以在github下载，有兴趣可以试一下。\n",
    "\n",
    "# 更复杂的图像识别\n",
    "会涉及到很多不同的网络层，大多是卷积神经网络的扩展，由于比较复杂，只说一下它们的使用。\n",
    "\n",
    "我们使用复杂的模型目的是要识别自己的图像，自己的类别，那么从网上下载来的模型是在其他类别上训练的，和我们的物品类别不同，这里有一个方法叫做迁移学习。\n",
    "\n",
    "## 迁移学习\n",
    "\n",
    "其实迁移学习在人类学习中是占了很大比重的，比如学习汉字，认识了一个偏旁，再看到某个字有这个偏旁，虽然不认识，也可以猜个大概。\n",
    "机器学习的迁移学习中，底层网络可以提取低级的特征，就像刚才说的偏旁，那我们重做上层网络，保留下层已经训练好的参数，这就是迁移学习。\n",
    "\n",
    "比如现在想做英文字母的识别，手头有英文字母的图像，但是图像非常少每样就几十张，直接训练很容易过拟合。cnn_mnist的模型，准确率已经很好了，那就可以借它的网络一用，假设字母图像也是单通道28\\*28的图像，那我们基础网络不需要改变，只需把输出个数修改，并且从数字模型中恢复除了最后一层以外的训练好的参数并且固定住禁止训练，只有最后一层允许训练，这样的话结果会比直接拿字母数据集训练的结果更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 复杂模型\n",
    "\n",
    "## Inception_v3\n",
    "\n",
    "Inception_v3是谷歌提出的一种神经网络。由于网络结构比较复杂，这里就介绍一下怎么迁移学习。\n",
    "Inception_v3的网络最后三层是一个全连接层+一个Dropout层+一个全连接层。\n",
    "\n",
    "```\n",
    "dense = tf.layers.dense(inputs=net, units=1024, activation=tf.nn.relu)  # 这里的activation是激活函数，是一个非线性函数\n",
    "dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "predictions = tf.layers.dense(inputs=dropout, units=params.get(\"nb_classes\"), name=\"predictions\")  # 最后一层的输出数量就是类别个数\n",
    "```\n",
    "\n",
    "relu：修正线性单元（Rectified Linear Unit）\n",
    "```\n",
    "def relu(x):\n",
    "    return max(0, x)  # 最常用的就是max\n",
    "```\n",
    "\n",
    "![relu](./images/sigmoid_and_relu.png)\n",
    "\n",
    "我们修改这最后的一层或两层全连接层就能够完成迁移学习，因为低级特征被保存在卷积层中，我们迁移学习不会修改那些参数。\n",
    "\n",
    "先看看修改最后一层的，我们可以在模型中写死\n",
    "\n",
    "```\n",
    "prediction = tf.layers.dense(inputs=dropout, units=42)  # 我有42个类别需要识别\n",
    "```\n",
    "\n",
    "也可以通过model_fn的第四个形参`params`,一个字典，传入units个数，这样model_fn不需要经常修改。\n",
    "\n",
    "然后在返回训练模式的`EstimatorSpec`之前，从训练好的参数中加载参数，当然，这里需要注意，因为要重新训练最后一层，所以最后一层的参数不能加载，而是让Estimator去自动初始化。\n",
    "\n",
    "```\n",
    "exclude = [\"predictions\"]  # 最后一层的名字，因为需要训练新参数，所以这层要排除\n",
    "# 这里使用了Tensorflow的一个高级API包slim，获取需要恢复的参数(变量)列表\n",
    "variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "tf.train.init_from_checkpoint(params.get(\"checkpoint\"), {v.name.split(':')[0]: v for v in variables_to_restore})  # 将checkpoint路径传入，第二个参数是变量名映射，checkpoint中的变量名和静态图中的变量名有差别。这个例子中差别在于变量名后面的 :0 标记。\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么一个迁移学习模型就做好了，再构造一个输入函数，将图片和对应的标签传入estimator就可以训练了，Tensorflow也提供了一个可视化工具**Tensorboard**可以在浏览器中方便地查看损失函数值变化趋势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
